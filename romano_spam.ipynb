{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02789576",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries needed\n",
    "import numpy as np      #used for algebra\n",
    "import pandas as pd     #used for reading csv files\n",
    "import nltk             #used to apply statistics to language\n",
    "import tensorflow as tf #used for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58c60807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     classification                                            message\n",
      "0               ham  Go until jurong point, crazy.. Available only ...\n",
      "1               ham                      Ok lar... Joking wif u oni...\n",
      "2              spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3               ham  U dun say so early hor... U c already then say...\n",
      "4               ham  Nah I don't think he goes to usf, he lives aro...\n",
      "...             ...                                                ...\n",
      "5567           spam  This is the 2nd time we have tried 2 contact u...\n",
      "5568            ham              Will ÃŒ_ b going to esplanade fr home?\n",
      "5569            ham  Pity, * was in mood for that. So...any other s...\n",
      "5570            ham  The guy did some bitching but I acted like i'd...\n",
      "5571            ham                         Rofl. Its true to its name\n",
      "\n",
      "[5572 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#here we call the csv files that we will use\n",
    "datatrain = pd.read_csv('spam.csv',encoding='latin-1')\n",
    "datatest = pd.read_csv('output_spam.csv')\n",
    "#we drop the columns of the csv file that are empty\n",
    "datatrain = datatrain.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "datatrain = datatrain.rename(columns={\"v1\":'classification', \"v2\":'message'})\n",
    "#these are used to change the labels in the csv file\n",
    "tags = datatrain[\"classification\"]\n",
    "texts = datatrain[\"message\"]\n",
    "\n",
    "print(datatrain) #this shows us what is inside the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b883488f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classification</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       classification                 message\n",
       "count            5572                    5572\n",
       "unique              2                    5169\n",
       "top               ham  Sorry, I'll call later\n",
       "freq             4825                      30"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datatrain.describe() #this shows us the number of words inside the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93488c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     classification                                            message\n",
      "102             ham  As per your request 'Melle Melle (Oru Minnamin...\n",
      "153             ham  As per your request 'Melle Melle (Oru Minnamin...\n",
      "206             ham  As I entered my cabin my PA said, '' Happy B'd...\n",
      "222             ham                             Sorry, I'll call later\n",
      "325             ham                   No calls..messages..missed calls\n",
      "...             ...                                                ...\n",
      "5524           spam  You are awarded a SiPix Digital Camera! call 0...\n",
      "5535            ham  I know you are thinkin malaria. But relax, chi...\n",
      "5539            ham                         Just sleeping..and surfing\n",
      "5553            ham                        Hahaha..use your brain dear\n",
      "5558            ham                             Sorry, I'll call later\n",
      "\n",
      "[403 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "dupe = datatrain[datatrain.duplicated()] #this check for duplicates in the dataset\n",
    "print(dupe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11df4e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classification</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5169</td>\n",
       "      <td>5169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4516</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       classification                                            message\n",
       "count            5169                                               5169\n",
       "unique              2                                               5169\n",
       "top               ham  Go until jurong point, crazy.. Available only ...\n",
       "freq             4516                                                  1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datatrain.drop_duplicates(inplace = True) #this is used to drop the duplicates in the dataset\n",
    "datatrain.describe() #use to check the count again and as you can see it reduced since we dropped the duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "722fbc82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classification    0\n",
       "message           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datatrain.isnull().sum() #used to check for empty/null cells in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d58ba26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5169, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datatrain.shape #used to check the array shape/size of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ebcf5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS #library used to store words and determine the characteristics\n",
    "#used to segregate the words based on their classification\n",
    "hams = datatrain[datatrain.classification==\"ham\"]\n",
    "spams = datatrain[datatrain.classification==\"spam\"]\n",
    "#this turns the segregated words into a numpy array to be used later\n",
    "hams_new = \" \".join(hams.message.to_numpy().tolist())\n",
    "spams_new = \" \".join(spams.message.to_numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc74aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordcloud of ham messages\n",
    "hams_cloud = WordCloud(width =520, height =260, stopwords=STOPWORDS,max_font_size=50, background_color =\"black\", colormap='Blues').generate(hams_new)\n",
    "#wordcloud of spam messages\n",
    "spams_cloud = WordCloud(width =520, height =260, stopwords=STOPWORDS,max_font_size=50, background_color =\"black\", colormap='Blues').generate(spams_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2f121be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_12580\\267994691.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  cm = hams_msg.append(spams_msg).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "#used to change the length and make it so that both ham and spam have the same length\n",
    "hams_msg = hams.sample(n = len(spams), random_state = 44)\n",
    "spams_msg = spams\n",
    "cm = hams_msg.append(spams_msg).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65d9bb6",
   "metadata": {},
   "source": [
    "hams_msg = hams.sample(n=len(spams), random_state = 44)\n",
    "spams_msg = spams\n",
    "cm = hams_msg.append(spams_msg).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27d97c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is used to change the value of ham to 0 and spam to 1 so that the values will be integers and not string\n",
    "cm[\"msg_class\"] = cm[\"classification\"].map({\"ham\": 0, \"spam\": 1})\n",
    "cm_class = cm[\"msg_class\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae9b49b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the train-test split library and used to split the dataset into the test and train variables\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(cm[\"message\"], cm_class, test_size=0.2, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64a5672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer #library used to change the strings into tokens\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences #library used to format the string\n",
    "#this converts the words into individual tokens and stored in an index\n",
    "token = Tokenizer(num_words=500, char_level=False, oov_token=\"<OOV>\")\n",
    "token.fit_on_texts(X_train)\n",
    "index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fee8a4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training sequencing and padding, a safe way of multiprocessing the data in the dataset\n",
    "train_sequence = token.texts_to_sequences(X_train)\n",
    "train_pad = pad_sequences(train_sequence, maxlen=50, padding=\"post\", truncating=\"post\")\n",
    "#testing sequencing and padding, a safe way of multiprocessing the data in the dataset\n",
    "test_sequence = token.texts_to_sequences(X_test)\n",
    "test_pad = pad_sequences(test_sequence, maxlen=50,padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e135752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "33/33 [==============================] - 1s 9ms/step - loss: 0.6893 - accuracy: 0.5728 - val_loss: 0.6799 - val_accuracy: 0.8397\n",
      "Epoch 2/12\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.6657 - accuracy: 0.8563 - val_loss: 0.6391 - val_accuracy: 0.8893\n",
      "Epoch 3/12\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.6036 - accuracy: 0.8745 - val_loss: 0.5493 - val_accuracy: 0.8969\n",
      "Epoch 4/12\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4956 - accuracy: 0.8879 - val_loss: 0.4392 - val_accuracy: 0.9046\n",
      "Epoch 5/12\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.3811 - accuracy: 0.8994 - val_loss: 0.3403 - val_accuracy: 0.9160\n",
      "Epoch 6/12\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2944 - accuracy: 0.9119 - val_loss: 0.2758 - val_accuracy: 0.9198\n",
      "Epoch 7/12\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2314 - accuracy: 0.9291 - val_loss: 0.2364 - val_accuracy: 0.9275\n",
      "Epoch 8/12\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1918 - accuracy: 0.9444 - val_loss: 0.2045 - val_accuracy: 0.9275\n",
      "Epoch 9/12\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1584 - accuracy: 0.9464 - val_loss: 0.1851 - val_accuracy: 0.9313\n",
      "Epoch 10/12\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1392 - accuracy: 0.9540 - val_loss: 0.1715 - val_accuracy: 0.9389\n",
      "Epoch 11/12\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.1224 - accuracy: 0.9665 - val_loss: 0.1635 - val_accuracy: 0.9313\n",
      "Epoch 12/12\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1079 - accuracy: 0.9665 - val_loss: 0.1557 - val_accuracy: 0.9389\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a396290a60>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential #library used to import the model to be used\n",
    "from tensorflow.keras.layers import Flatten, Dense, Activation, Dropout, Embedding, GlobalAveragePooling1D #library used to import the layers of the neural network and the other needed function such as Flatten and Activation\n",
    "#this is the model of the neural network starting from the input layer upto the output layer\n",
    "model = Sequential()\n",
    "model.add(Embedding(500, 16, input_length=50)) #captures the semantics of the input and makes it easier to do machine learning\n",
    "model.add(GlobalAveragePooling1D()) #Functions the same as Flatten, however, GlobalAveragePooling is better with larger variables\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "#the model compiler used to finalize the model and get it ready for use in fitting and predicting\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(train_pad, y_train, epochs=12, validation_data=(test_pad, y_test)) #model fitting is the training and validation of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8d1dec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 92ms/step\n"
     ]
    }
   ],
   "source": [
    "test_msg = datatest.iloc[:,1] #calling the test dataset\n",
    "def predict_spam(test_msg): #defining the spam prediction function\n",
    "    new_seq = token.texts_to_sequences(test_msg) #creating a token for the prediction function\n",
    "    padded = pad_sequences(new_seq, maxlen=50, padding=\"post\", truncating=\"post\") #just like in the training variables, the prediction variables need to be padded as well\n",
    "    return (model.predict(padded)) #returns the predicted value\n",
    "\n",
    "prediction_final = predict_spam(test_msg) #predicts the input text, in this case, the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a4e6962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is used to round off the float values into 0 and 1 int values to make the data easier to understand, 1 for spam, 0 for ham\n",
    "pred = np.round_(prediction_final, decimals = 0, out = None)\n",
    "pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
